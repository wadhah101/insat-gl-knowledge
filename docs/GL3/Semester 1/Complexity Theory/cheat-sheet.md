---
slug: /gl3/semester-1/complexity-theory/cheat-sheet
---

# Cheat sheet

Author [@rihemebh](https://github.com/rihemebh)


## Time Complexity

### Introduction ? 
<aside>
ğŸ’¡ The time complexity of an algorithm is the total amount of time required by an algorithm to complete its execution.

</aside>
<br/>

![assets/what/Untitled.png](assets/what/Untitled.png)

### **1)**Â O(1)â€Šâ€”â€ŠConstant Time

Constant time means the running time is constant, itâ€™sÂ *not affected by the input size (i.e.*Â Basic Operations (arithmetic, comparisons, accessing arrayâ€™s elements, assignment*))*

Example :

```c
read(x)    // O(1)
a = 10;    // O(1)
a = 1.000.000.000.000.000.000 // O(1)
```

### **2)**Â O(n)â€Šâ€”â€ŠLinear Time

When an algorithm accepts n input size, it would perform n operations as well.

Consider the following example, below I am linearly searching for an element, this has a time complexity of O(n) because it goes through the loop n times.

```c
int find = 66;
var numbers = new int[] { 33, 435, 36, 37, 43, 45, 66, 656, 2232 };
for (int i = 0; i < numbers.Length - 1; i++) {
    if(find == numbers[i]) {
        return;
    }
}
```

### **3)**Â O(log n) â€” Logarithmic Time

Algorithm that has running time O(log n) is slight faster than O(n). Commonly, algorithm divides the problem into sub problems with the same size. Example: binary search algorithm, binary conversion algorithm.

![assets/what/Untitled1.png](assets/what/Untitled1.png)

```
+-----------+--------------------+
| Iteration  |     Value of M     |        
+-----------+--------------------+
| 1         | n                  | 
| 2         | n/2                |
| 3         | n/2^2              | 
| 4         | n/2^3              |
| .         |  .                 |
| .         |  .                 |
| k         | n/2^k              |
+-----------+--------------------+
```

loop will run k times so that m>1.

*m>1 ; putting value of m for as $n/2^k$*

$*n/2^k >1*$

$*2^k > n*$

*k > logn*

### **4)**Â O(n log n) â€” Linearithmic Time

This running time is often found in â€œdivide & conquer algorithmsâ€ which divide the problem into sub problems recursively and then merge them in n time. Example: Merge Sort algorithm.

example:

![what/Untitled%202.png](assets/what/Untitled2.png)

![https://miro.medium.com/max/1268/1*WkgYyMhCnAKDk8ZrqYxufQ.png](https://miro.medium.com/max/1268/1*WkgYyMhCnAKDk8ZrqYxufQ.png)

so n+n/2+n/3+..+1 times total loop will run

n(1+1/2+1/3+1/4) = nlogn

so time complexity is nlogn.

### **5)**Â O(nÂ²)â€Šâ€”â€ŠQuadratic Time

For example Bubble Sort algorithm, in other words, aÂ loopÂ insideÂ aÂ loopÂ :

```c
for(i=0; i < N; i++) {
  for(j=0; j < N;j++) { 
    statement;
  }
}
```

### **6)**Â O(nÂ³) â€” Cubic Time

It has the same principle as O(nÂ²).

### **7)**Â O(2^n) â€” Exponential Time

It is very slow as input get larger, if n = 1000.000, T(n) would be 21000.000. Brute Force algorithm has this running time.

An algorithm is said to have an exponential time complexity when the growth doubles with each addition to the input data set.

```c
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
```

### **8)**Â O(n!) â€”Factorial Time

Its the slowest of them all.

An algorithm is said to have a factorial time complexity when it grows in a factorial way based on the size of the input data

Example: Travel Salesman Problem (TSP), Heap Permutation

```python
def heap_permutation(data, n):
    if n == 1:
        print(data)
        return

    for i in range(n):
        heap_permutation(data, n - 1)
        if n % 2 == 0:
            data[i], data[n-1] = data[n-1], data[i]
        else:
            data[0], data[n-1] = data[n-1], data[0]
```

![assets/what/Untitled%203.png](assets/what/Untitled3.png)



## Cours ComplexitÃ© (robbena)


### Algorithme

Un ensemble dâ€™instructions qui transforme un ensemble de donnÃ©es en un ensemble de rÃ©sultats en un nombre fini de taches. => Utilisation de la mÃ©moire et le temps

### Algorithme polynomiale

Un algorithme est polynomial si pour un certain k> 0, son temps d'exÃ©cution sur les entrÃ©es de taille n est O ($n ^ k$). Cela inclut linÃ©aire, quadratique, cubique et plus. D'un autre cÃ´tÃ©, les algorithmes Ã  temps d'exÃ©cution exponentiel ne sont pas polynomiaux.

### Algorithme efficace 

la compexitÃ© est au maxium polynomiale

 Un algorithme est dit efficace si sa complexitÃ© asymptotique est dans O (P(n)) ou P(n) est une fonction polynomiale en fonction de donnÃ©e

un algorithme A est meilleur que B ssiÂ : $t_A (n)=O(t_B (n))$  Et   $t_B (n) â‰ O(t_A (n))$

### Algorithme NP-complet (wikipedia)
Un algorithme NP-complet est un problÃ¨me de dÃ©cision vÃ©rifiant les propriÃ©tÃ©s suivantes :

- il est possible de vÃ©rifier une solution efficacement (en temps polynomial) ; la classe des problÃ¨mes vÃ©rifiant cette propriÃ©tÃ© est notÃ©e NP ;
- tous les problÃ¨mes de la classe NP se ramÃ¨nent Ã  celui-ci via une rÃ©duction polynomiale ; cela signifie que le problÃ¨me est au moins aussi difficile que tous les autres problÃ¨mes de la classe NP.

### Pire cas, meilleur cas, cas moyen

- Meilleur casÂ : Tmin(n)
- Pire cas : Tmax(n)
- Cas Moyen:  $Tmoy (n)= âˆ‘^n _i ã€–p_i*t(i)ã€—$: p probabilitÃ©

---

### ComplexitÃ© asymptotique

- Le dÃ©compte dÃ©taillÃ© de nombre dâ€™instruction peut Ãªtre compliquÃ© engendrant une expression qui nÃ©cessite une approximation. En plus le temps dâ€™exÃ©cution des instructions Ã©lÃ©mentaires est diffÃ©rent dâ€™une machine Ã  une autre. DÃ©finitionÂ : La complexitÃ© asymptotique dÃ©crit le comportement dâ€™un algorithme quand la taille de donnÃ©e n devient de plus en plus grande plus tout quâ€™une mesure exacte (sorte de ignorance des constante devant les termes variant en fonction de n)
- **Grand â€“ O**Â : soit T(n) une fonction non nÃ©gative, elle est sâ€™il existe deux constantes positive c et  $n_0$ telles queÂ :

![assets/Summary/Untitled.png](assets/Summary/Untitled.png)

- **Grand - Î©:** soit T(n) une fonction non nÃ©gative, elle est sâ€™il existe deux constantes positive c et $n_0$Â telles queÂ :

![assets/Summary/Untitled%201.png](assets/Summary/Untitled%201.png)

- **Grand â€“ Î¸**Â : pour une fonction T(n) no nÃ©gative si son grand â€“ O et son grand - Î© coÃ¯ncide alors on parle dâ€™un Grand â€“ Î¸

<aside>
ğŸ“ RemarquesÂ : 
 - Le meilleur cas nâ€™est pas le cas ou la taille est fixÃ©e Ã  1 
- Le pire cas nâ€™a rien Ã  avoir avec la complexitÃ© asymptotique

</aside>

---

### RecursivitÃ©

[Recursion](Recursion)

- RÃ©cursivitÃ© simpleÂ : une fonction qui sâ€™appel Ã  elle-mÃªme
- RÃ©cursivitÃ© croisÃ©eÂ : F1 qui appelle F2 et F2 qui appelle F1
- RÃ©cursivitÃ© terminaleÂ : Appel rÃ©cursive terminant le corps de la fonction
- RÃ©cursivitÃ© non terminaleÂ : fonction Comporte un traitement aprÃ¨s lâ€™appel rÃ©cursive

1- Les appels rÃ©cursifs de la fonction doivent sâ€™arrÃªter Ã  un moment donnÃ© (test dâ€™arrÃªt)

2- Un processus de rÃ©ductionÂ : on doit avec chaque appel de rapprocher de la condition dâ€™arrÃªt

---

### Divide and Conquer

  Des algorithmes Ã  structures rÃ©cursives devisant le problÃ¨me initial en des problÃ¨mes similaires de tailles moindres, les rÃ©soudre, puis les combiner pour rÃ©soudre le problÃ¨me initial. (Diviser, RÃ©gner, Combiner)

ComplexitÃ©:

$t(n)= a*t(n/b)+f(n)$ 

a est le nombre d'appelavec la taille n/b

 f(n)est le cout de la combinaison

Ainsi : si f(n)=Î¸(n^a ) avec a>0 alors

![assets/Summary/Untitled%202.png](assets/Summary/Untitled%202.png)

Un algorithme de taille n contenant un appel rÃ©cursif, son temps dâ€™exÃ©cution est dÃ©crit par une Ã©quation de rÃ©currence en fonction de temps dâ€™exÃ©cution pour des entrÃ©es de taille moindre

La rÃ©currence dÃ©finissant le temps dâ€™exÃ©cution de lâ€™algorithme se dÃ©compose en 2 casÂ :

1- si la taille de donnÃ©es n est suffisamment rÃ©duite

2- si on dÃ©compose le problÃ¨me en a sous problÃ¨mes de taille 1/b la taille initiale on aÂ :

$T(n)= D(n)+aT(n/b)+C(n)$

D temps necessairepour la decomposition

T temps de resoudre probleme

C temps de composition de la solution finale

---
### Simplification
### Les regles :

![assets/Simplification/Untitled.png](assets/Simplification/Untitled.png)

![assets/Simplification/Untitled%201.png](assets/Simplification/Untitled%201.png)

![assets/Simplification/Untitled%202.png](assets/Simplification/Untitled%202.png)

![assets/Simplification/Untitled%203.png](assets/Simplification/Untitled%203.png)

### Pour calculer la complexitÃ© d'un algorithme :

![assets/Simplification/Untitled%204.png](assets/Simplification/Untitled%204.png)

![assets/Simplification/Untitled%205.png](assets/Simplification/Untitled%205.png)

---

![assets/Simplification/Untitled%206.png](assets/Simplification/Untitled%206.png)

![assets/Simplification/Untitled%207.png](assets/Simplification/Untitled%207.png)
### Normalisation des boucles

**Boucle for arithmÃ©tique:** for (i=k; i<=n; i+=c)  ****

**Boucle for normalisÃ©e:** for (j=0; j<=nb-1; j++)

- Le nombre dâ€™itÃ©ration de la boucle arithmÃ©tique est ($E((n-k)/c+1)$ si  $`k>n`$
- En transformant on retranche une valeur Îµ de la borne sup de la boucle tel que

La transformation de la boucle for (i = supÂ ; i infÂ ; i-= dec) en  for (i = infÂ ; iÂ supÂ ; i+=dec) garde le mÃªme nombre dâ€™itÃ©rations

```c
i = supÂ ;
 while (i > inf) 
{
 //instructions ne modifiant ni inf ni sup ni decÂ ; 
i-=decÂ ;
 }
```

- est Ã©quivalente en nombre dâ€™itÃ©rations Ã  la boucle  `for (i = supÂ ; i infÂ ; i-= dec)`

(de mÃªme si on inverse sup est inf et on changeant la dÃ©crÃ©mentation en incrÃ©mentation)

- La boucle â€˜â€™ `do while`Â â€˜â€™ est Ã©quivalente Ã  une boucle while prÃ©cÃ©dÃ©e par lâ€™exÃ©cution de son corps une fois

**Boucles gÃ©omÃ©triques** : `for (i=inf ; i â‰¤ sup â€“ Îµ ; i*=c)`

![assets/Summary/Untitled%203.png](assets/Summary/Untitled%203.png)

Comme dans le cas des boucles arithmÃ©tique, les boucles gÃ©omÃ©triques prÃ©cÃ©dentes ont des Ã©quivalentes par  `while`  et `do while` 

![assets/Summary/Untitled%204.png](assets/Summary/Untitled%204.png)


### Master Theorem

 **The Master Method is used for solving the following types of recurrence**


```
procedure T( n : size of problem ) defined as:
If n < 1 then exit
Do work of amount f(n)
T(n/b)
T(n/b)
â€¦repeat for a total of a timesâ€¦
T(n/b)
end procedure**
```


â‡’ **T (n) = aÂ T (n/b) + f (n)**Â 

with aâ‰¥1 

bâ‰¥1 constant 

 f(n)  function

Let T (n) is defined on non-negative integers by the recurrence.

- n is the size of the problem.
- a is the number of sub problems in the recursion. (nb appel de la fonction)
- n/b is the size of each sub problem. (Here it is assumed that all sub problems are essentially the same size.)
- f (n) is the time to create the sub problems and combine their results in the above procedure
## Complexity

![assets/MasterTheorem/Untitled.png](assets/MasterTheorem/Untitled.png)

![assets/MasterTheorem/Untitled%201.png](assets/MasterTheorem/Untitled%201.png)

![assets/MasterTheorem/Untitled%202.png](assets/MasterTheorem/Untitled%202.png)

![assets/MasterTheorem/Untitled%203.png](assets/MasterTheorem/Untitled%203.png)

The master theorem cannot be used if:

- T(n)Â is not monotone. eg.Â `T(n) = sin n`
- `f(n)`Â is not a polynomial. eg.Â `f(n) = 2n`
- aÂ is not a constant. eg.Â `a = 2n`
- `a < 1`



## Useful Videos / playlist

- https://www.youtube.com/channel/UCZCFT11CWBi3MHNlGf019nw
- https://www.youtube.com/watch?v=2H0GKdrIowU&ab_channel=Rman

